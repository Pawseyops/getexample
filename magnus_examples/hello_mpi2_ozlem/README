#!/bin/bash
# This is the README file is an executable script.
# To run type: ./README
#
# Example OBjECTIVE: to demostrate hello_mpi.f90 example on 
# Magnus with two partially occupied nodes.
# To run this code, specify the total number of MPI tasks,
# the number of MPI tasks per node
# and the number of MPI tasks per socket
# This information is located under hello_mpi2.slurm
# You can edit the SLURM as: emacs hello_mpi2.slurm &

# SLURM directives
# 
# Here we specify to SLURM we want 2 node (--nodes=2)
# To launch a job, we specify to aprun 24 MPI tasks (-n 24)
# with 12 MPI tasks per node (-N 12)
# and 6 MPI tasks per socket (-S 6)
# Therefore the aprun command: aprun -n 24 -N 12 -S 6

# To compile the hello_mpi.f90 code
ftn -O2 hello_mpi.f90 -o hello_mpi2

# To submit the job to Magnus
sbatch hello_mpi2.slurm

echo "The sbatch command returns what jobid is for this job."
echo "To check the status of your job, use the slurm command:"
echo "squeue -u $USER"
echo "Your job is deleted from the scratch."
echo "It is now moved to your group."
echo "Your results are now located in ${MYGROUP}/hello_mpi2_results/"
echo "To change to your jobID directory, type:"
echo "cd ${MYGROUP}/hello_mpi2_results/jobID/"
echo "To view the results, use the cat command and type:"
echo "cat hello_mpi2.log"
