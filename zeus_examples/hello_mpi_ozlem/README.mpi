#!/bin/bash
# This is the README file is an executable script.
# To run type: ./README
#
# Example OBjECTIVE: to demostrate hello_mpi.f90 example
# on Zeus. 
# This example is taken from:
# https://people.sc.fsu.edu/~jburkardt/f_src/hello_mpi/hello_mpi.html
# To run this code, edit the partition, load the necessary modules
# and specify the total number of MPI tasks.
# This information is located under hello_world_mpi.slurm
# You can edit the SLURM as: emacs hello_world_mpi.slurm &

# SLURM directives
# 
# Here we specify to SLURM we want 2 nodes (--nodes=2)
# Modify the partition to --partition==workq
# To compile the code with intel swap from gcc:
# module swap gcc intel
# Load the necessary modules before module listing.
# module load mpt
# We specify to srun that 32 MPI tasks are required. (-n 32)
# with 2 nodes (-N 2) given that 
# there are 16 MPI tasks per node.
# For the correct operation, MPI also needs to be specified to srun
# via the option "--mpi=pmi2"
# Therefore, the srun command should look like:
# srun --mpi=pmi2 -n 32 -N 2 ./$EXECUTABLE >> ${OUTPUT}
 

# To compile the hello_mpi.f90 code
mpif90 hello_mpi.f90 -I${FPATH} -o hello_mpi_zeus

# To submit the job to Magnus
sbatch hello_world_mpi.slurm

echo "The sbatch command returns what jobid is for this job."
echo "To check the status of your job, use the slurm command:"
echo "squeue -u $USER"
echo "Your job is deleted from the scratch."
echo "It is now moved to your group."
echo "Your results are now located in ${MYGROUP}/hello_results_zeus/"
echo "To change to your jobID directory, type:"
echo "cd ${MYGROUP}/hello_results_zeus/jobID/"
echo "To view the results, use the cat command and type:"
echo "cat hello_mpi_zeus.log"
